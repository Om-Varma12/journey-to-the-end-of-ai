{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e604535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "from mcp import ClientSession\n",
    "from mcp.client.stdio import stdio_client\n",
    "from mcp.client.streamable_http import streamable_http_client\n",
    "\n",
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c565b3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    messages: list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "149ffd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "510af8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def connect_to_servers():\n",
    "    sessions = []\n",
    "\n",
    "    # stdio servers\n",
    "    stdio_servers = [\n",
    "        [\"python\", \"add_server.py\"],\n",
    "        [\"python\", \"multiply_server.py\"],\n",
    "    ]\n",
    "\n",
    "    for cmd in stdio_servers:\n",
    "        transport = await stdio_client(cmd)\n",
    "        session = await ClientSession.create(transport)\n",
    "        sessions.append(session)\n",
    "\n",
    "    # streamable-http server\n",
    "    http_transport = await streamable_http_client(\n",
    "        \"http://127.0.0.1:8000\"\n",
    "    )\n",
    "    http_session = await ClientSession.create(http_transport)\n",
    "    sessions.append(http_session)\n",
    "\n",
    "    return sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76f4deca",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def agent_node(state: State):\n",
    "    sessions = await connect_to_servers()\n",
    "\n",
    "    tools = []\n",
    "    for s in sessions:\n",
    "        tools.extend(await s.list_tools())\n",
    "\n",
    "    model = llm.bind_tools(tools)\n",
    "\n",
    "    response = await model.ainvoke(state[\"messages\"])\n",
    "\n",
    "    return {\"messages\": state[\"messages\"] + [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c14f7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(State)\n",
    "builder.add_node(\"agent\", agent_node)\n",
    "builder.add_edge(START, \"agent\")\n",
    "builder.add_edge(\"agent\", END)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e3d4b30",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object _AsyncGeneratorContextManager can't be used in 'await' expression",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m result[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m     13\u001b[39m         \u001b[38;5;28mprint\u001b[39m(m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m main()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m():\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m graph.ainvoke(\n\u001b[32m      3\u001b[39m         {\n\u001b[32m      4\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m      5\u001b[39m                 HumanMessage(\n\u001b[32m      6\u001b[39m                     content=\u001b[33m\"\u001b[39m\u001b[33mWhat is 4 + 6? Then multiply result by 5. Also tell weather.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m                 )\n\u001b[32m      8\u001b[39m             ]\n\u001b[32m      9\u001b[39m         }\n\u001b[32m     10\u001b[39m     )\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m result[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m     13\u001b[39m         \u001b[38;5;28mprint\u001b[39m(m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\STUDY\\PYTHON\\journey-to-the-end-of-ai\\LangGraph\\MCP Tutorial\\.venv\\Lib\\site-packages\\langgraph\\pregel\\main.py:3161\u001b[39m, in \u001b[36mPregel.ainvoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3158\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3159\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3161\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.astream(\n\u001b[32m   3162\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   3163\u001b[39m     config,\n\u001b[32m   3164\u001b[39m     context=context,\n\u001b[32m   3165\u001b[39m     stream_mode=[\u001b[33m\"\u001b[39m\u001b[33mupdates\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   3166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3167\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m stream_mode,\n\u001b[32m   3168\u001b[39m     print_mode=print_mode,\n\u001b[32m   3169\u001b[39m     output_keys=output_keys,\n\u001b[32m   3170\u001b[39m     interrupt_before=interrupt_before,\n\u001b[32m   3171\u001b[39m     interrupt_after=interrupt_after,\n\u001b[32m   3172\u001b[39m     durability=durability,\n\u001b[32m   3173\u001b[39m     **kwargs,\n\u001b[32m   3174\u001b[39m ):\n\u001b[32m   3175\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   3176\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) == \u001b[32m2\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\STUDY\\PYTHON\\journey-to-the-end-of-ai\\LangGraph\\MCP Tutorial\\.venv\\Lib\\site-packages\\langgraph\\pregel\\main.py:2974\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2972\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop.amatch_cached_writes():\n\u001b[32m   2973\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2974\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   2975\u001b[39m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   2976\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   2977\u001b[39m     get_waiter=get_waiter,\n\u001b[32m   2978\u001b[39m     schedule_task=loop.aaccept_push,\n\u001b[32m   2979\u001b[39m ):\n\u001b[32m   2980\u001b[39m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m _output(\n\u001b[32m   2982\u001b[39m         stream_mode,\n\u001b[32m   2983\u001b[39m         print_mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2986\u001b[39m         asyncio.QueueEmpty,\n\u001b[32m   2987\u001b[39m     ):\n\u001b[32m   2988\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\STUDY\\PYTHON\\journey-to-the-end-of-ai\\LangGraph\\MCP Tutorial\\.venv\\Lib\\site-packages\\langgraph\\pregel\\_runner.py:304\u001b[39m, in \u001b[36mPregelRunner.atick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    302\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[32m    305\u001b[39m         t,\n\u001b[32m    306\u001b[39m         retry_policy,\n\u001b[32m    307\u001b[39m         stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    308\u001b[39m         configurable={\n\u001b[32m    309\u001b[39m             CONFIG_KEY_CALL: partial(\n\u001b[32m    310\u001b[39m                 _acall,\n\u001b[32m    311\u001b[39m                 weakref.ref(t),\n\u001b[32m    312\u001b[39m                 stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    313\u001b[39m                 retry_policy=retry_policy,\n\u001b[32m    314\u001b[39m                 futures=weakref.ref(futures),\n\u001b[32m    315\u001b[39m                 schedule_task=schedule_task,\n\u001b[32m    316\u001b[39m                 submit=\u001b[38;5;28mself\u001b[39m.submit,\n\u001b[32m    317\u001b[39m                 loop=loop,\n\u001b[32m    318\u001b[39m             ),\n\u001b[32m    319\u001b[39m         },\n\u001b[32m    320\u001b[39m     )\n\u001b[32m    321\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\STUDY\\PYTHON\\journey-to-the-end-of-ai\\LangGraph\\MCP Tutorial\\.venv\\Lib\\site-packages\\langgraph\\pregel\\_retry.py:138\u001b[39m, in \u001b[36marun_with_retry\u001b[39m\u001b[34m(task, retry_policy, stream, match_cached_writes, configurable)\u001b[39m\n\u001b[32m    136\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task.proc.ainvoke(task.input, config)\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    140\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\STUDY\\PYTHON\\journey-to-the-end-of-ai\\LangGraph\\MCP Tutorial\\.venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:705\u001b[39m, in \u001b[36mRunnableSeq.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    703\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    704\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m705\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(\n\u001b[32m    706\u001b[39m             step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs), context=context\n\u001b[32m    707\u001b[39m         )\n\u001b[32m    708\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    709\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\STUDY\\PYTHON\\journey-to-the-end-of-ai\\LangGraph\\MCP Tutorial\\.venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:473\u001b[39m, in \u001b[36mRunnableCallable.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    471\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_chain_end(ret)\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.afunc(*args, **kwargs)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    475\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m ret.ainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36magent_node\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34magent_node\u001b[39m(state: State):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     sessions = \u001b[38;5;28;01mawait\u001b[39;00m connect_to_servers()\n\u001b[32m      4\u001b[39m     tools = []\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m sessions:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mconnect_to_servers\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      5\u001b[39m stdio_servers = [\n\u001b[32m      6\u001b[39m     [\u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33madd_server.py\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      7\u001b[39m     [\u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmultiply_server.py\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      8\u001b[39m ]\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cmd \u001b[38;5;129;01min\u001b[39;00m stdio_servers:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     transport = \u001b[38;5;28;01mawait\u001b[39;00m stdio_client(cmd)\n\u001b[32m     12\u001b[39m     session = \u001b[38;5;28;01mawait\u001b[39;00m ClientSession.create(transport)\n\u001b[32m     13\u001b[39m     sessions.append(session)\n",
      "\u001b[31mTypeError\u001b[39m: object _AsyncGeneratorContextManager can't be used in 'await' expression",
      "During task with name 'agent' and id '8951b7aa-39bf-d17c-75d0-9daff9078f4e'"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "    result = await graph.ainvoke(\n",
    "        {\n",
    "            \"messages\": [\n",
    "                HumanMessage(\n",
    "                    content=\"What is 4 + 6? Then multiply result by 5. Also tell weather.\"\n",
    "                )\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    for m in result[\"messages\"]:\n",
    "        print(m)\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e3a279",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MCP Tutorial (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
